{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Trace Generator Benchmark\n",
        "\n",
        "Generate business process traces using TraceGenerator and compare them with the original event log using SimulationBenchmark.\n",
        "\n",
        "**Workflow:**\n",
        "1. Load trained model and initialize TraceGenerator\n",
        "2. Generate traces from various case attributes\n",
        "3. Load original event log for comparison\n",
        "4. Compare using SimulationBenchmark\n",
        "5. Visualize key differences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "# Add project root to path\n",
        "project_root = Path(__file__).parent.parent.parent.parent if '__file__' in globals() else Path.cwd().parent.parent.parent\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "# Add Next-Activity-Prediction to path\n",
        "na_root = project_root / \"Next-Activity-Prediction\"\n",
        "if str(na_root) not in sys.path:\n",
        "    sys.path.insert(0, str(na_root))\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from bpic17_simplified import TraceGenerator\n",
        "from integration.SimulationBenchmark import SimulationBenchmark\n",
        "\n",
        "try:\n",
        "    import pm4py\n",
        "    PM4PY_AVAILABLE = True\n",
        "except ImportError:\n",
        "    PM4PY_AVAILABLE = False\n",
        "    warnings.warn(\"pm4py not available. XES file loading will not work.\")\n",
        "\n",
        "pd.options.mode.chained_assignment = None\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "print(f\"Benchmarking started: {datetime.now()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Paths\n",
        "MODEL_PATH = os.path.join(project_root, \"models\", \"bpic17_simplified\")\n",
        "EVENT_LOG_PATH = os.path.join(project_root, \"eventlog\", \"eventlog.xes.gz\")\n",
        "OUTPUT_DIR = os.path.join(project_root, \"integration\", \"output\", \"trace_generator\")\n",
        "\n",
        "# Trace generation parameters\n",
        "NUM_TRACES = 200\n",
        "MAX_TRACE_LENGTH = 100\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"Model path: {MODEL_PATH}\")\n",
        "print(f\"Output directory: {OUTPUT_DIR}\")\n",
        "print(f\"Number of traces to generate: {NUM_TRACES}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Original Event Log\n",
        "where the "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not PM4PY_AVAILABLE:\n",
        "    print(\"⚠ pm4py not available. Cannot load XES file.\")\n",
        "    print(\"Please install pm4py: pip install pm4py\")\n",
        "else:\n",
        "    print(\"Loading original event log...\")\n",
        "    original_log = pm4py.read_xes(EVENT_LOG_PATH)\n",
        "    df_original = pm4py.convert_to_dataframe(original_log)\n",
        "    \n",
        "    # Filter to start/complete only (matching our model)\n",
        "    df_original = df_original[df_original['lifecycle:transition'].isin(['start', 'complete'])].copy()\n",
        "    \n",
        "    # Sample cases for comparison\n",
        "    unique_cases = df_original['case:concept:name'].unique()\n",
        "    if len(unique_cases) > NUM_TRACES:\n",
        "        np.random.seed(42)\n",
        "        sampled_cases = np.random.choice(unique_cases, NUM_TRACES, replace=False)\n",
        "        df_original = df_original[df_original['case:concept:name'].isin(sampled_cases)].copy()\n",
        "    \n",
        "    original_csv = os.path.join(OUTPUT_DIR, \"original_log.csv\")\n",
        "    df_original.to_csv(original_csv, index=False)\n",
        "    print(f\"✓ Original log loaded: {len(df_original)} events, {df_original['case:concept:name'].nunique()} cases\")\n",
        "    print(f\"  Saved to: {original_csv}\")\n",
        "    \n",
        "    # Extract case attribute distributions for realistic trace generation\n",
        "    case_attributes_dist = df_original.groupby('case:concept:name').first()\n",
        "    loan_goals = case_attributes_dist['case:LoanGoal'].dropna().unique().tolist()\n",
        "    app_types = case_attributes_dist['case:ApplicationType'].dropna().unique().tolist()\n",
        "    amounts = case_attributes_dist['case:RequestedAmount'].dropna().tolist()\n",
        "    \n",
        "    print(f\"\\nFound case attributes:\")\n",
        "    print(f\"  Loan goals: {len(loan_goals)} unique values\")\n",
        "    print(f\"  Application types: {len(app_types)} unique values\")\n",
        "    print(f\"  Requested amounts: min={min(amounts):.0f}, max={max(amounts):.0f}, mean={np.mean(amounts):.0f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Initialize Trace Generator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Loading TraceGenerator...\")\n",
        "generator = TraceGenerator(\n",
        "    model_path=MODEL_PATH,\n",
        "    max_trace_length=MAX_TRACE_LENGTH,\n",
        "    seed=42\n",
        ")\n",
        "print(\"✓ TraceGenerator loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Generate Traces\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate case attributes based on original log distribution\n",
        "np.random.seed(42)\n",
        "\n",
        "case_attributes = []\n",
        "for i in range(NUM_TRACES):\n",
        "    loan_goal = np.random.choice(loan_goals) if loan_goals else \"Home improvement\"\n",
        "    app_type = np.random.choice(app_types) if app_types else \"New credit\"\n",
        "    amount = np.random.choice(amounts) if amounts else 10000.0\n",
        "    \n",
        "    case_attributes.append({\n",
        "        \"loan_goal\": loan_goal,\n",
        "        \"application_type\": app_type,\n",
        "        \"requested_amount\": float(amount),\n",
        "        \"case_id\": f\"generated_case_{i+1:04d}\"\n",
        "    })\n",
        "\n",
        "print(f\"Generating {NUM_TRACES} traces...\")\n",
        "print(\"This may take a few minutes...\")\n",
        "\n",
        "traces = generator.generate_traces(case_attributes)\n",
        "\n",
        "print(f\"✓ Generated {len(traces)} traces\")\n",
        "print(f\"  Total events: {sum(len(t) for t in traces)}\")\n",
        "print(f\"  Average trace length: {np.mean([len(t) for t in traces]):.1f} activities\")\n",
        "print(f\"  Min trace length: {min(len(t) for t in traces)} activities\")\n",
        "print(f\"  Max trace length: {max(len(t) for t in traces)} activities\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Convert Traces to Event Log Format\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Converting traces to event log format...\")\n",
        "df_generated = generator.traces_to_dataframe(traces)\n",
        "\n",
        "generated_csv = os.path.join(OUTPUT_DIR, \"generated_log.csv\")\n",
        "df_generated.to_csv(generated_csv, index=False)\n",
        "\n",
        "print(f\"✓ Generated log saved: {len(df_generated)} events, {df_generated['case:concept:name'].nunique()} cases\")\n",
        "print(f\"  Saved to: {generated_csv}\")\n",
        "\n",
        "print(f\"\\nSample generated events (first 5):\")\n",
        "for idx, row in df_generated.head().iterrows():\n",
        "    print(f\"  [{row['time:timestamp']}] {row['case:concept:name']}: {row['concept:name']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Run Benchmark Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not PM4PY_AVAILABLE:\n",
        "    print(\"⚠ Cannot run benchmark without pm4py. Skipping...\")\n",
        "else:\n",
        "    print(\"=\" * 60)\n",
        "    print(\"TRACE GENERATOR BENCHMARK\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Original log: {original_csv}\")\n",
        "    print(f\"Generated log: {generated_csv}\")\n",
        "    print(\"=\" * 60)\n",
        "    print()\n",
        "    \n",
        "    benchmark = SimulationBenchmark(original_csv, generated_csv)\n",
        "    \n",
        "    print(\"Computing metrics...\")\n",
        "    results = benchmark.compute_all_metrics()\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"SUMMARY\")\n",
        "    print(\"=\" * 60)\n",
        "    benchmark.print_summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Visualize Key Metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not PM4PY_AVAILABLE:\n",
        "    print(\"⚠ Cannot visualize without pm4py. Skipping...\")\n",
        "else:\n",
        "    # Set up plotting style\n",
        "    plt.style.use('seaborn-v0_8')\n",
        "    sns.set_palette(\"husl\")\n",
        "    \n",
        "    # 1. Events per case distribution\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    \n",
        "    # Events per case\n",
        "    orig_events_per_case = df_original.groupby('case:concept:name').size()\n",
        "    gen_events_per_case = df_generated.groupby('case:concept:name').size()\n",
        "    \n",
        "    axes[0, 0].hist(orig_events_per_case, bins=30, alpha=0.7, label='Original', density=True)\n",
        "    axes[0, 0].hist(gen_events_per_case, bins=30, alpha=0.7, label='Generated', density=True)\n",
        "    axes[0, 0].set_xlabel('Events per Case')\n",
        "    axes[0, 0].set_ylabel('Density')\n",
        "    axes[0, 0].set_title('Events per Case Distribution')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Activity frequency (top 20)\n",
        "    orig_act_freq = df_original['concept:name'].value_counts().head(20)\n",
        "    gen_act_freq = df_generated['concept:name'].value_counts().head(20)\n",
        "    \n",
        "    all_activities = list(set(orig_act_freq.index) | set(gen_act_freq.index))\n",
        "    orig_counts = [orig_act_freq.get(act, 0) for act in all_activities]\n",
        "    gen_counts = [gen_act_freq.get(act, 0) for act in all_activities]\n",
        "    \n",
        "    x = np.arange(len(all_activities))\n",
        "    width = 0.35\n",
        "    axes[0, 1].bar(x - width/2, orig_counts, width, label='Original', alpha=0.7)\n",
        "    axes[0, 1].bar(x + width/2, gen_counts, width, label='Generated', alpha=0.7)\n",
        "    axes[0, 1].set_xlabel('Activity')\n",
        "    axes[0, 1].set_ylabel('Frequency')\n",
        "    axes[0, 1].set_title('Top 20 Activity Frequencies')\n",
        "    axes[0, 1].set_xticks(x)\n",
        "    axes[0, 1].set_xticklabels(all_activities, rotation=45, ha='right')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    # Start activities\n",
        "    orig_starts = df_original.groupby('case:concept:name').first()['concept:name'].value_counts()\n",
        "    gen_starts = df_generated.groupby('case:concept:name').first()['concept:name'].value_counts()\n",
        "    \n",
        "    all_starts = list(set(orig_starts.index) | set(gen_starts.index))\n",
        "    orig_start_counts = [orig_starts.get(act, 0) for act in all_starts]\n",
        "    gen_start_counts = [gen_starts.get(act, 0) for act in all_starts]\n",
        "    \n",
        "    x = np.arange(len(all_starts))\n",
        "    axes[1, 0].bar(x - width/2, orig_start_counts, width, label='Original', alpha=0.7)\n",
        "    axes[1, 0].bar(x + width/2, gen_start_counts, width, label='Generated', alpha=0.7)\n",
        "    axes[1, 0].set_xlabel('Start Activity')\n",
        "    axes[1, 0].set_ylabel('Frequency')\n",
        "    axes[1, 0].set_title('Start Activity Distribution')\n",
        "    axes[1, 0].set_xticks(x)\n",
        "    axes[1, 0].set_xticklabels(all_starts, rotation=45, ha='right')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    # End activities\n",
        "    orig_ends = df_original.groupby('case:concept:name').last()['concept:name'].value_counts()\n",
        "    gen_ends = df_generated.groupby('case:concept:name').last()['concept:name'].value_counts()\n",
        "    \n",
        "    all_ends = list(set(orig_ends.index) | set(gen_ends.index))\n",
        "    orig_end_counts = [orig_ends.get(act, 0) for act in all_ends]\n",
        "    gen_end_counts = [gen_ends.get(act, 0) for act in all_ends]\n",
        "    \n",
        "    x = np.arange(len(all_ends))\n",
        "    axes[1, 1].bar(x - width/2, orig_end_counts, width, label='Original', alpha=0.7)\n",
        "    axes[1, 1].bar(x + width/2, gen_end_counts, width, label='Generated', alpha=0.7)\n",
        "    axes[1, 1].set_xlabel('End Activity')\n",
        "    axes[1, 1].set_ylabel('Frequency')\n",
        "    axes[1, 1].set_title('End Activity Distribution')\n",
        "    axes[1, 1].set_xticks(x)\n",
        "    axes[1, 1].set_xticklabels(all_ends, rotation=45, ha='right')\n",
        "    axes[1, 1].legend()\n",
        "    axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plot_path = os.path.join(OUTPUT_DIR, \"benchmark_visualizations.png\")\n",
        "    plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
        "    print(f\"✓ Visualizations saved to: {plot_path}\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Export Benchmark Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not PM4PY_AVAILABLE:\n",
        "    print(\"⚠ Cannot export results without pm4py. Skipping...\")\n",
        "else:\n",
        "    output_path = os.path.join(OUTPUT_DIR, \"benchmark_results.xlsx\")\n",
        "    print(f\"Exporting results to: {output_path}\")\n",
        "    benchmark.export_results(output_path)\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"BENCHMARK COMPLETE\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Results saved to: {output_path}\")\n",
        "    print(f\"Generated log saved to: {generated_csv}\")\n",
        "    print(f\"Original log saved to: {original_csv}\")\n",
        "    print(f\"Benchmarking completed at: {datetime.now()}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
