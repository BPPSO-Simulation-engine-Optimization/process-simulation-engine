{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Comprehensive Evaluation: Unified Next Activity Predictor\n",
        "\n",
        "Evaluate the unified next activity predictor against BPIC 2017 event log with lifecycle filtering.\n",
        "\n",
        "**Evaluation Metrics:**\n",
        "- Activity prediction accuracy (overall, top-k, per-class)\n",
        "- Lifecycle prediction accuracy\n",
        "- Joint accuracy (both activity and lifecycle correct)\n",
        "- Confusion matrices and classification reports\n",
        "- Trace-level performance analysis\n",
        "- Baseline comparisons\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation started: 2026-01-06 22:43:53.700964\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Repos\\process-simulation-engine\\.venv\\Lib\\site-packages\\keras\\src\\export\\tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
            "  if not hasattr(np, \"object\"):\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import warnings\n",
        "import importlib\n",
        "from pathlib import Path\n",
        "\n",
        "# Add parent to path\n",
        "sys.path.insert(0, os.path.dirname(os.getcwd()))\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics import (\n",
        "    classification_report, \n",
        "    confusion_matrix, \n",
        "    accuracy_score,\n",
        "    precision_recall_fscore_support\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
        "from pm4py.objects.conversion.log import converter as log_converter\n",
        "\n",
        "# Force reload modules to pick up changes\n",
        "import unified\n",
        "import unified.model\n",
        "import unified.data_generator\n",
        "import unified.utils\n",
        "importlib.reload(unified.model)\n",
        "importlib.reload(unified.data_generator)\n",
        "importlib.reload(unified.utils)\n",
        "importlib.reload(unified)\n",
        "\n",
        "from unified import UnifiedDataGenerator, UnifiedPredictor, UnifiedModelPersistence, filter_lifecycle_events\n",
        "\n",
        "pd.options.mode.chained_assignment = None\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
        "\n",
        "print(f\"Evaluation started: {datetime.now()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model directory: d:\\Repos\\process-simulation-engine\\Next-Activity-Prediction\\advanced\\notebooks\\..\\..\\..\\models\\unified_next_activity\n",
            "Output directory: d:\\Repos\\process-simulation-engine\\Next-Activity-Prediction\\advanced\\notebooks\\..\\..\\..\\results\\unified_evaluation\n",
            "Test set size: 20%\n"
          ]
        }
      ],
      "source": [
        "# Paths\n",
        "XES_PATH = os.path.join(os.getcwd(), \"..\", \"..\", \"..\", \"Dataset\", \"BPI Challenge 2017.xes\")\n",
        "MODEL_DIR = os.path.join(os.getcwd(), \"..\", \"..\", \"..\", \"models\", \"unified_next_activity\")\n",
        "OUTPUT_DIR = os.path.join(os.getcwd(), \"..\", \"..\", \"..\", \"results\", \"unified_evaluation\")\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Evaluation parameters\n",
        "TEST_SIZE = 0.2  # Last 20% of cases by timestamp for temporal split\n",
        "MAX_HISTORY = 20  # Match training configuration\n",
        "TOP_K_VALUES = [1, 3, 5]  # For top-k accuracy\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "print(f\"Model directory: {MODEL_DIR}\")\n",
        "print(f\"Output directory: {OUTPUT_DIR}\")\n",
        "print(f\"Test set size: {TEST_SIZE*100:.0f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Trained Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model from: d:\\Repos\\process-simulation-engine\\Next-Activity-Prediction\\advanced\\notebooks\\..\\..\\..\\models\\unified_next_activity\n",
            "WARNING:tensorflow:From d:\\Repos\\process-simulation-engine\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "\n",
            "Model loaded successfully!\n",
            "  Target activities: 25\n",
            "  Target lifecycles: 2\n",
            "  Context keys: ['case:LoanGoal', 'case:ApplicationType', 'case:RequestedAmount']\n",
            "  Max sequence length: 20\n",
            "\n",
            "Available activities (25):\n",
            "['A_Accepted' 'A_Cancelled' 'A_Complete' 'A_Concept' 'A_Denied'\n",
            " 'A_Incomplete' 'A_Pending' 'A_Submitted' 'A_Validating' 'End'] ...\n",
            "\n",
            "Available lifecycles (2):\n",
            "['complete' 'start']\n"
          ]
        }
      ],
      "source": [
        "print(f\"Loading model from: {MODEL_DIR}\")\n",
        "bundle = UnifiedModelPersistence.load(MODEL_DIR)\n",
        "model = bundle[\"model\"]\n",
        "encoder = bundle[\"encoder\"]\n",
        "\n",
        "print(f\"\\nModel loaded successfully!\")\n",
        "print(f\"  Target activities: {encoder.num_target_activities}\")\n",
        "print(f\"  Target lifecycles: {encoder.num_target_lifecycles}\")\n",
        "print(f\"  Context keys: {encoder.context_keys}\")\n",
        "print(f\"  Max sequence length: {encoder.max_seq_len}\")\n",
        "\n",
        "# Display available activities and lifecycles\n",
        "print(f\"\\nAvailable activities ({len(encoder.target_activity_encoder.classes_)}):\")\n",
        "print(encoder.target_activity_encoder.classes_[:10], \"...\" if len(encoder.target_activity_encoder.classes_) > 10 else \"\")\n",
        "\n",
        "print(f\"\\nAvailable lifecycles ({len(encoder.target_lifecycle_encoder.classes_)}):\")\n",
        "print(encoder.target_lifecycle_encoder.classes_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load and Filter Event Log\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading event log from: d:\\Repos\\process-simulation-engine\\Next-Activity-Prediction\\advanced\\notebooks\\..\\..\\..\\Dataset\\BPI Challenge 2017.xes\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "parsing log, completed traces :: 100%|██████████| 31509/31509 [00:39<00:00, 801.12it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 1,202,267 events from 31,509 cases\n",
            "\n",
            "Columns: ['Action', 'org:resource', 'concept:name', 'EventOrigin', 'EventID', 'lifecycle:transition', 'time:timestamp', 'case:LoanGoal', 'case:ApplicationType', 'case:concept:name', 'case:RequestedAmount', 'FirstWithdrawalAmount', 'NumberOfTerms', 'Accepted', 'MonthlyCost', 'Selected', 'CreditScore', 'OfferedAmount', 'OfferID']\n",
            "\n",
            "Lifecycle transitions before filtering:\n",
            "lifecycle:transition\n",
            "complete     475306\n",
            "suspend      215402\n",
            "schedule     149104\n",
            "start        128227\n",
            "resume       127160\n",
            "ate_abort     85224\n",
            "withdraw      21844\n",
            "Name: count, dtype: int64\n",
            "\n",
            "After filtering: 603,533 events from 31,509 cases\n"
          ]
        }
      ],
      "source": [
        "print(f\"Loading event log from: {XES_PATH}\")\n",
        "event_log = xes_importer.apply(XES_PATH)\n",
        "df_log = log_converter.apply(event_log, variant=log_converter.Variants.TO_DATA_FRAME)\n",
        "\n",
        "print(f\"Loaded {len(df_log):,} events from {df_log['case:concept:name'].nunique():,} cases\")\n",
        "print(f\"\\nColumns: {list(df_log.columns)}\")\n",
        "\n",
        "# Check lifecycle distribution before filtering\n",
        "if 'lifecycle:transition' in df_log.columns:\n",
        "    print(f\"\\nLifecycle transitions before filtering:\")\n",
        "    print(df_log['lifecycle:transition'].value_counts())\n",
        "    \n",
        "    # Apply lifecycle filtering (keep only 'start' and 'complete')\n",
        "    df_log = filter_lifecycle_events(df_log, allowed_lifecycles=['start', 'complete'])\n",
        "    print(f\"\\nAfter filtering: {len(df_log):,} events from {df_log['case:concept:name'].nunique():,} cases\")\n",
        "else:\n",
        "    print(\"\\nWarning: No 'lifecycle:transition' column found. All events will be kept.\")\n",
        "\n",
        "# Ensure timestamp is datetime\n",
        "df_log[\"time:timestamp\"] = pd.to_datetime(df_log[\"time:timestamp\"])\n",
        "\n",
        "# Sort by case and timestamp\n",
        "df_log = df_log.sort_values([\"case:concept:name\", \"time:timestamp\"]).reset_index(drop=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Prepare Test Set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Temporal split:\n",
            "  Training cases: 25,207 (481,714 events)\n",
            "  Test cases: 6,302 (121,819 events)\n",
            "  Test period: 2016-10-18 23:42:39.265000+00:00 to 2017-02-01 14:00:30.347000+00:00\n",
            "\n",
            "Generated 121,819 test sequences for evaluation\n"
          ]
        }
      ],
      "source": [
        "# Temporal split: use last TEST_SIZE% of cases by timestamp\n",
        "df_log[\"time:timestamp\"] = pd.to_datetime(df_log[\"time:timestamp\"])\n",
        "case_start_times = df_log.groupby(\"case:concept:name\")[\"time:timestamp\"].min().sort_values()\n",
        "split_idx = int(len(case_start_times) * (1 - TEST_SIZE))\n",
        "test_cases = set(case_start_times.iloc[split_idx:].index)\n",
        "\n",
        "df_test = df_log[df_log[\"case:concept:name\"].isin(test_cases)].copy()\n",
        "df_train = df_log[~df_log[\"case:concept:name\"].isin(test_cases)].copy()\n",
        "\n",
        "print(f\"Temporal split:\")\n",
        "print(f\"  Training cases: {len(df_train['case:concept:name'].unique()):,} ({len(df_train):,} events)\")\n",
        "print(f\"  Test cases: {len(df_test['case:concept:name'].unique()):,} ({len(df_test):,} events)\")\n",
        "print(f\"  Test period: {df_test['time:timestamp'].min()} to {df_test['time:timestamp'].max()}\")\n",
        "\n",
        "# Prepare test sequences (similar to training data generation but without filtering)\n",
        "# We'll evaluate at each position in each trace\n",
        "test_sequences = []\n",
        "\n",
        "for case_id, group in df_test.groupby(\"case:concept:name\"):\n",
        "    activities = group[\"concept:name\"].tolist()\n",
        "    lifecycles = group[\"lifecycle:transition\"].fillna(\"complete\").str.lower().tolist()\n",
        "    resources = group[\"org:resource\"].fillna(\"Unknown\").tolist()\n",
        "    timestamps = group[\"time:timestamp\"].tolist()\n",
        "    \n",
        "    # Extract context\n",
        "    context = {}\n",
        "    for key in encoder.context_keys:\n",
        "        if key in group.columns:\n",
        "            context[key] = group[key].iloc[0]\n",
        "        else:\n",
        "            context[key] = None\n",
        "    \n",
        "    # Add END token at the end\n",
        "    activities.append(\"End\")\n",
        "    lifecycles.append(\"complete\")\n",
        "    resources.append(\"Unknown\")\n",
        "    if timestamps:\n",
        "        timestamps.append(timestamps[-1])\n",
        "    \n",
        "    # Create sequences for evaluation (skip first event, need at least 2)\n",
        "    if len(activities) < 2:\n",
        "        continue\n",
        "    \n",
        "    for i in range(1, len(activities)):\n",
        "        start_idx = max(0, i - MAX_HISTORY)\n",
        "        seq_activities = activities[start_idx:i]\n",
        "        seq_lifecycles = lifecycles[start_idx:i]\n",
        "        seq_resources = resources[start_idx:i]\n",
        "        seq_timestamps = timestamps[start_idx:i]\n",
        "        \n",
        "        # Compute durations\n",
        "        durations = [0.0]\n",
        "        for j in range(1, len(seq_timestamps)):\n",
        "            delta = (seq_timestamps[j] - seq_timestamps[j-1]).total_seconds()\n",
        "            durations.append(np.log1p(max(0.0, delta)))\n",
        "        \n",
        "        test_sequences.append({\n",
        "            \"case_id\": case_id,\n",
        "            \"position\": i,\n",
        "            \"sequence_activities\": seq_activities,\n",
        "            \"sequence_lifecycles\": seq_lifecycles,\n",
        "            \"sequence_resources\": seq_resources,\n",
        "            \"sequence_durations\": durations,\n",
        "            \"target_activity\": activities[i],\n",
        "            \"target_lifecycle\": lifecycles[i],\n",
        "            \"context\": context,\n",
        "        })\n",
        "\n",
        "print(f\"\\nGenerated {len(test_sequences):,} test sequences for evaluation\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running evaluation loop...\n",
            "Evaluating 121,819 sequences...\n",
            "  Progress: 6,090/121,819 (5.0%)\n",
            "  Progress: 12,180/121,819 (10.0%)\n",
            "  Progress: 18,270/121,819 (15.0%)\n"
          ]
        }
      ],
      "source": [
        "print(\"Running evaluation loop...\")\n",
        "print(f\"Evaluating {len(test_sequences):,} sequences...\")\n",
        "\n",
        "# Storage for predictions and ground truth\n",
        "all_pred_activities = []\n",
        "all_pred_lifecycles = []\n",
        "all_true_activities = []\n",
        "all_true_lifecycles = []\n",
        "all_top_k_activities = {k: [] for k in TOP_K_VALUES}\n",
        "\n",
        "# Track errors\n",
        "errors = []\n",
        "\n",
        "# Progress tracking\n",
        "eval_every = max(1, len(test_sequences) // 20)\n",
        "\n",
        "for idx, seq in enumerate(test_sequences):\n",
        "    if (idx + 1) % eval_every == 0:\n",
        "        print(f\"  Progress: {idx+1:,}/{len(test_sequences):,} ({100*(idx+1)/len(test_sequences):.1f}%)\")\n",
        "    \n",
        "    try:\n",
        "        # Predict\n",
        "        activity_probs, lifecycle_probs = model.predict(\n",
        "            seq[\"sequence_activities\"],\n",
        "            seq[\"sequence_lifecycles\"],\n",
        "            seq[\"sequence_resources\"],\n",
        "            seq[\"sequence_durations\"],\n",
        "            seq[\"context\"],\n",
        "            top_k=max(TOP_K_VALUES)\n",
        "        )\n",
        "        \n",
        "        # Get top-1 predictions\n",
        "        pred_activity = activity_probs[0][0] if activity_probs else \"UNKNOWN\"\n",
        "        pred_lifecycle = lifecycle_probs[0][0] if lifecycle_probs else \"complete\"\n",
        "        \n",
        "        # Get top-k activities\n",
        "        for k in TOP_K_VALUES:\n",
        "            top_k_acts = [act for act, _ in activity_probs[:k]]\n",
        "            all_top_k_activities[k].append(top_k_acts)\n",
        "        \n",
        "        # Store predictions\n",
        "        all_pred_activities.append(pred_activity)\n",
        "        all_pred_lifecycles.append(pred_lifecycle)\n",
        "        all_true_activities.append(seq[\"target_activity\"])\n",
        "        all_true_lifecycles.append(seq[\"target_lifecycle\"])\n",
        "        \n",
        "    except Exception as e:\n",
        "        errors.append({\n",
        "            \"case_id\": seq[\"case_id\"],\n",
        "            \"position\": seq[\"position\"],\n",
        "            \"error\": str(e)\n",
        "        })\n",
        "        # Use fallback predictions\n",
        "        all_pred_activities.append(\"UNKNOWN\")\n",
        "        all_pred_lifecycles.append(\"complete\")\n",
        "        all_true_activities.append(seq[\"target_activity\"])\n",
        "        all_true_lifecycles.append(seq[\"target_lifecycle\"])\n",
        "        for k in TOP_K_VALUES:\n",
        "            all_top_k_activities[k].append([])\n",
        "\n",
        "print(f\"\\nEvaluation complete!\")\n",
        "print(f\"  Successful predictions: {len(test_sequences) - len(errors):,}\")\n",
        "print(f\"  Errors: {len(errors):,}\")\n",
        "\n",
        "if errors:\n",
        "    print(f\"\\nSample errors:\")\n",
        "    for err in errors[:5]:\n",
        "        print(f\"  Case {err['case_id']}, position {err['position']}: {err['error']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert to numpy arrays for easier computation\n",
        "y_true_act = np.array(all_true_activities)\n",
        "y_pred_act = np.array(all_pred_activities)\n",
        "y_true_lc = np.array(all_true_lifecycles)\n",
        "y_pred_lc = np.array(all_pred_lifecycles)\n",
        "\n",
        "# Overall accuracy\n",
        "activity_accuracy = accuracy_score(y_true_act, y_pred_act)\n",
        "lifecycle_accuracy = accuracy_score(y_true_lc, y_pred_lc)\n",
        "\n",
        "# Joint accuracy (both correct)\n",
        "joint_correct = (y_true_act == y_pred_act) & (y_true_lc == y_pred_lc)\n",
        "joint_accuracy = joint_correct.mean()\n",
        "\n",
        "# Top-k accuracy\n",
        "top_k_accuracies = {}\n",
        "for k in TOP_K_VALUES:\n",
        "    correct = 0\n",
        "    for i, true_act in enumerate(y_true_act):\n",
        "        if true_act in all_top_k_activities[k][i]:\n",
        "            correct += 1\n",
        "    top_k_accuracies[k] = correct / len(y_true_act)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"OVERALL METRICS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Activity Accuracy:     {activity_accuracy:.4f} ({activity_accuracy*100:.2f}%)\")\n",
        "print(f\"Lifecycle Accuracy:    {lifecycle_accuracy:.4f} ({lifecycle_accuracy*100:.2f}%)\")\n",
        "print(f\"Joint Accuracy:        {joint_accuracy:.4f} ({joint_accuracy*100:.2f}%)\")\n",
        "print(f\"\\nTop-K Activity Accuracy:\")\n",
        "for k in TOP_K_VALUES:\n",
        "    print(f\"  Top-{k}: {top_k_accuracies[k]:.4f} ({top_k_accuracies[k]*100:.2f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Per-activity metrics\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ACTIVITY PREDICTION METRICS\")\n",
        "print(\"=\" * 60)\n",
        "activity_report = classification_report(\n",
        "    y_true_act, y_pred_act,\n",
        "    output_dict=True,\n",
        "    zero_division=0\n",
        ")\n",
        "\n",
        "print(\"\\nClassification Report (Activities):\")\n",
        "print(classification_report(y_true_act, y_pred_act, zero_division=0))\n",
        "\n",
        "# Per-lifecycle metrics\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"LIFECYCLE PREDICTION METRICS\")\n",
        "print(\"=\" * 60)\n",
        "lifecycle_report = classification_report(\n",
        "    y_true_lc, y_pred_lc,\n",
        "    output_dict=True,\n",
        "    zero_division=0\n",
        ")\n",
        "\n",
        "print(\"\\nClassification Report (Lifecycles):\")\n",
        "print(classification_report(y_true_lc, y_pred_lc, zero_division=0))\n",
        "\n",
        "# Conditional accuracies\n",
        "act_given_lc_correct = (y_true_act == y_pred_act)[y_true_lc == y_pred_lc]\n",
        "lc_given_act_correct = (y_true_lc == y_pred_lc)[y_true_act == y_pred_act]\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"CONDITIONAL ACCURACIES\")\n",
        "print(\"=\" * 60)\n",
        "if len(act_given_lc_correct) > 0:\n",
        "    print(f\"Activity accuracy given lifecycle correct: {act_given_lc_correct.mean():.4f} ({act_given_lc_correct.mean()*100:.2f}%)\")\n",
        "else:\n",
        "    print(\"Activity accuracy given lifecycle correct: N/A (no lifecycle matches)\")\n",
        "    \n",
        "if len(lc_given_act_correct) > 0:\n",
        "    print(f\"Lifecycle accuracy given activity correct: {lc_given_act_correct.mean():.4f} ({lc_given_act_correct.mean()*100:.2f}%)\")\n",
        "else:\n",
        "    print(\"Lifecycle accuracy given activity correct: N/A (no activity matches)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Baseline Comparisons\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Most frequent baseline\n",
        "most_freq_activity = pd.Series(y_true_act).mode()[0] if len(pd.Series(y_true_act).mode()) > 0 else \"UNKNOWN\"\n",
        "most_freq_lifecycle = pd.Series(y_true_lc).mode()[0] if len(pd.Series(y_true_lc).mode()) > 0 else \"complete\"\n",
        "\n",
        "baseline_act_pred = np.full_like(y_true_act, most_freq_activity)\n",
        "baseline_lc_pred = np.full_like(y_true_lc, most_freq_lifecycle)\n",
        "\n",
        "baseline_act_acc = accuracy_score(y_true_act, baseline_act_pred)\n",
        "baseline_lc_acc = accuracy_score(y_true_lc, baseline_lc_pred)\n",
        "baseline_joint_acc = ((y_true_act == baseline_act_pred) & (y_true_lc == baseline_lc_pred)).mean()\n",
        "\n",
        "# Random baseline (uniform distribution)\n",
        "np.random.seed(RANDOM_STATE)\n",
        "unique_activities = np.unique(y_true_act)\n",
        "unique_lifecycles = np.unique(y_true_lc)\n",
        "\n",
        "random_act_pred = np.random.choice(unique_activities, size=len(y_true_act))\n",
        "random_lc_pred = np.random.choice(unique_lifecycles, size=len(y_true_lc))\n",
        "\n",
        "random_act_acc = accuracy_score(y_true_act, random_act_pred)\n",
        "random_lc_acc = accuracy_score(y_true_lc, random_lc_pred)\n",
        "random_joint_acc = ((y_true_act == random_act_pred) & (y_true_lc == random_lc_pred)).mean()\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"BASELINE COMPARISONS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nMost Frequent Baseline:\")\n",
        "print(f\"  Activity Accuracy:     {baseline_act_acc:.4f} ({baseline_act_acc*100:.2f}%)\")\n",
        "print(f\"  Lifecycle Accuracy:    {baseline_lc_acc:.4f} ({baseline_lc_acc*100:.2f}%)\")\n",
        "print(f\"  Joint Accuracy:        {baseline_joint_acc:.4f} ({baseline_joint_acc*100:.2f}%)\")\n",
        "\n",
        "print(f\"\\nRandom Baseline:\")\n",
        "print(f\"  Activity Accuracy:     {random_act_acc:.4f} ({random_act_acc*100:.2f}%)\")\n",
        "print(f\"  Lifecycle Accuracy:     {random_lc_acc:.4f} ({random_lc_acc*100:.2f}%)\")\n",
        "print(f\"  Joint Accuracy:        {random_joint_acc:.4f} ({random_joint_acc*100:.2f}%)\")\n",
        "\n",
        "print(f\"\\nModel vs Baselines:\")\n",
        "print(f\"  Activity improvement over most frequent: {((activity_accuracy - baseline_act_acc) / baseline_act_acc * 100):+.2f}%\")\n",
        "print(f\"  Activity improvement over random:       {((activity_accuracy - random_act_acc) / random_act_acc * 100):+.2f}%\")\n",
        "print(f\"  Joint improvement over most frequent:   {((joint_accuracy - baseline_joint_acc) / baseline_joint_acc * 100):+.2f}%\")\n",
        "print(f\"  Joint improvement over random:          {((joint_accuracy - random_joint_acc) / random_joint_acc * 100):+.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Visualizations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion matrix for activities\n",
        "cm_activities = confusion_matrix(y_true_act, y_pred_act, labels=encoder.target_activity_encoder.classes_)\n",
        "\n",
        "plt.figure(figsize=(14, 12))\n",
        "sns.heatmap(\n",
        "    cm_activities,\n",
        "    annot=True,\n",
        "    fmt=\"d\",\n",
        "    cmap=\"Blues\",\n",
        "    xticklabels=encoder.target_activity_encoder.classes_,\n",
        "    yticklabels=encoder.target_activity_encoder.classes_,\n",
        "    cbar_kws={\"label\": \"Count\"}\n",
        ")\n",
        "plt.title(\"Confusion Matrix: Activity Predictions\", fontsize=16, fontweight=\"bold\")\n",
        "plt.xlabel(\"Predicted Activity\", fontsize=12)\n",
        "plt.ylabel(\"True Activity\", fontsize=12)\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, \"confusion_matrix_activities.png\"), dpi=300, bbox_inches=\"tight\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion matrix for lifecycles\n",
        "cm_lifecycles = confusion_matrix(y_true_lc, y_pred_lc, labels=encoder.target_lifecycle_encoder.classes_)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(\n",
        "    cm_lifecycles,\n",
        "    annot=True,\n",
        "    fmt=\"d\",\n",
        "    cmap=\"Greens\",\n",
        "    xticklabels=encoder.target_lifecycle_encoder.classes_,\n",
        "    yticklabels=encoder.target_lifecycle_encoder.classes_,\n",
        "    cbar_kws={\"label\": \"Count\"}\n",
        ")\n",
        "plt.title(\"Confusion Matrix: Lifecycle Predictions\", fontsize=16, fontweight=\"bold\")\n",
        "plt.xlabel(\"Predicted Lifecycle\", fontsize=12)\n",
        "plt.ylabel(\"True Lifecycle\", fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, \"confusion_matrix_lifecycles.png\"), dpi=300, bbox_inches=\"tight\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Top-k accuracy comparison\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "k_values = [1] + TOP_K_VALUES\n",
        "acc_values = [activity_accuracy] + [top_k_accuracies[k] for k in TOP_K_VALUES]\n",
        "\n",
        "bars = ax.bar(range(len(k_values)), acc_values, color=\"steelblue\", alpha=0.7)\n",
        "ax.set_xlabel(\"Top-K\", fontsize=12)\n",
        "ax.set_ylabel(\"Accuracy\", fontsize=12)\n",
        "ax.set_title(\"Top-K Activity Prediction Accuracy\", fontsize=14, fontweight=\"bold\")\n",
        "ax.set_xticks(range(len(k_values)))\n",
        "ax.set_xticklabels([f\"Top-{k}\" for k in k_values])\n",
        "ax.set_ylim([0, 1])\n",
        "ax.grid(axis=\"y\", alpha=0.3)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, acc in zip(bars, acc_values):\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{acc:.3f}',\n",
        "            ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, \"top_k_accuracy.png\"), dpi=300, bbox_inches=\"tight\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Per-activity performance\n",
        "activity_performance = {}\n",
        "for activity in encoder.target_activity_encoder.classes_:\n",
        "    mask = y_true_act == activity\n",
        "    if mask.sum() > 0:\n",
        "        activity_performance[activity] = {\n",
        "            \"accuracy\": accuracy_score(y_true_act[mask], y_pred_act[mask]),\n",
        "            \"count\": mask.sum()\n",
        "        }\n",
        "\n",
        "# Sort by count and take top 15\n",
        "sorted_activities = sorted(activity_performance.items(), key=lambda x: x[1][\"count\"], reverse=True)[:15]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 8))\n",
        "activities = [a[0] for a in sorted_activities]\n",
        "accuracies = [a[1][\"accuracy\"] for a in sorted_activities]\n",
        "counts = [a[1][\"count\"] for a in sorted_activities]\n",
        "\n",
        "x_pos = np.arange(len(activities))\n",
        "bars = ax.barh(x_pos, accuracies, color=\"coral\", alpha=0.7)\n",
        "ax.set_yticks(x_pos)\n",
        "ax.set_yticklabels(activities)\n",
        "ax.set_xlabel(\"Accuracy\", fontsize=12)\n",
        "ax.set_title(\"Per-Activity Prediction Accuracy (Top 15 by Frequency)\", fontsize=14, fontweight=\"bold\")\n",
        "ax.set_xlim([0, 1])\n",
        "ax.grid(axis=\"x\", alpha=0.3)\n",
        "\n",
        "# Add count labels\n",
        "for i, (bar, count) in enumerate(zip(bars, counts)):\n",
        "    width = bar.get_width()\n",
        "    ax.text(width, bar.get_y() + bar.get_height()/2.,\n",
        "            f' {width:.3f} (n={count})',\n",
        "            ha='left', va='center', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, \"per_activity_accuracy.png\"), dpi=300, bbox_inches=\"tight\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Trace-Level Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze accuracy by position in trace\n",
        "position_accuracies = defaultdict(list)\n",
        "position_counts = defaultdict(int)\n",
        "\n",
        "for idx, seq in enumerate(test_sequences):\n",
        "    pos = seq[\"position\"]\n",
        "    position_counts[pos] += 1\n",
        "    if y_pred_act[idx] == y_true_act[idx]:\n",
        "        position_accuracies[pos].append(1)\n",
        "    else:\n",
        "        position_accuracies[pos].append(0)\n",
        "\n",
        "# Compute average accuracy per position\n",
        "pos_avg_acc = {pos: np.mean(accs) for pos, accs in position_accuracies.items() if len(accs) > 0}\n",
        "sorted_positions = sorted(pos_avg_acc.keys())\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "positions = sorted_positions[:50]  # Show first 50 positions\n",
        "accs = [pos_avg_acc[p] for p in positions]\n",
        "counts = [position_counts[p] for p in positions]\n",
        "\n",
        "ax.plot(positions, accs, marker=\"o\", linewidth=2, markersize=4, color=\"steelblue\")\n",
        "ax.set_xlabel(\"Position in Trace\", fontsize=12)\n",
        "ax.set_ylabel(\"Accuracy\", fontsize=12)\n",
        "ax.set_title(\"Prediction Accuracy by Position in Trace\", fontsize=14, fontweight=\"bold\")\n",
        "ax.set_ylim([0, 1])\n",
        "ax.grid(alpha=0.3)\n",
        "\n",
        "# Add count annotations for positions with low counts\n",
        "for pos, acc, count in zip(positions, accs, counts):\n",
        "    if count < 10:\n",
        "        ax.annotate(f\"n={count}\", (pos, acc), fontsize=7, alpha=0.6)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, \"accuracy_by_position.png\"), dpi=300, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "\n",
        "print(f\"Average accuracy by position (first 10 positions):\")\n",
        "for pos in sorted_positions[:10]:\n",
        "    print(f\"  Position {pos}: {pos_avg_acc[pos]:.4f} (n={position_counts[pos]})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Results Summary and Export\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create summary DataFrame\n",
        "summary_data = {\n",
        "    \"Metric\": [\n",
        "        \"Activity Accuracy\",\n",
        "        \"Lifecycle Accuracy\",\n",
        "        \"Joint Accuracy\",\n",
        "        f\"Top-{TOP_K_VALUES[0]} Accuracy\",\n",
        "        f\"Top-{TOP_K_VALUES[1]} Accuracy\",\n",
        "        f\"Top-{TOP_K_VALUES[2]} Accuracy\",\n",
        "        \"Baseline (Most Frequent) Activity Accuracy\",\n",
        "        \"Baseline (Random) Activity Accuracy\",\n",
        "        \"Baseline (Most Frequent) Joint Accuracy\",\n",
        "        \"Baseline (Random) Joint Accuracy\",\n",
        "    ],\n",
        "    \"Value\": [\n",
        "        activity_accuracy,\n",
        "        lifecycle_accuracy,\n",
        "        joint_accuracy,\n",
        "        top_k_accuracies[TOP_K_VALUES[0]],\n",
        "        top_k_accuracies[TOP_K_VALUES[1]],\n",
        "        top_k_accuracies[TOP_K_VALUES[2]],\n",
        "        baseline_act_acc,\n",
        "        random_act_acc,\n",
        "        baseline_joint_acc,\n",
        "        random_joint_acc,\n",
        "    ]\n",
        "}\n",
        "\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"SUMMARY METRICS\")\n",
        "print(\"=\" * 60)\n",
        "print(summary_df.to_string(index=False))\n",
        "\n",
        "# Save summary\n",
        "summary_df.to_csv(os.path.join(OUTPUT_DIR, \"summary_metrics.csv\"), index=False)\n",
        "print(f\"\\nSummary saved to: {os.path.join(OUTPUT_DIR, 'summary_metrics.csv')}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save detailed predictions\n",
        "predictions_df = pd.DataFrame({\n",
        "    \"case_id\": [seq[\"case_id\"] for seq in test_sequences],\n",
        "    \"position\": [seq[\"position\"] for seq in test_sequences],\n",
        "    \"true_activity\": y_true_act,\n",
        "    \"pred_activity\": y_pred_act,\n",
        "    \"true_lifecycle\": y_true_lc,\n",
        "    \"pred_lifecycle\": y_pred_lc,\n",
        "    \"correct_activity\": y_true_act == y_pred_act,\n",
        "    \"correct_lifecycle\": y_true_lc == y_pred_lc,\n",
        "    \"correct_joint\": joint_correct,\n",
        "})\n",
        "\n",
        "predictions_df.to_csv(os.path.join(OUTPUT_DIR, \"detailed_predictions.csv\"), index=False)\n",
        "print(f\"Detailed predictions saved to: {os.path.join(OUTPUT_DIR, 'detailed_predictions.csv')}\")\n",
        "print(f\"  Total predictions: {len(predictions_df):,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save per-activity metrics\n",
        "activity_metrics = []\n",
        "for activity in encoder.target_activity_encoder.classes_:\n",
        "    mask = y_true_act == activity\n",
        "    if mask.sum() > 0:\n",
        "        precision, recall, f1, support = precision_recall_fscore_support(\n",
        "            y_true_act[mask], y_pred_act[mask], average=\"weighted\", zero_division=0\n",
        "        )\n",
        "        acc = accuracy_score(y_true_act[mask], y_pred_act[mask])\n",
        "        activity_metrics.append({\n",
        "            \"activity\": activity,\n",
        "            \"accuracy\": acc,\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"f1_score\": f1,\n",
        "            \"support\": mask.sum()\n",
        "        })\n",
        "\n",
        "activity_metrics_df = pd.DataFrame(activity_metrics).sort_values(\"support\", ascending=False)\n",
        "activity_metrics_df.to_csv(os.path.join(OUTPUT_DIR, \"per_activity_metrics.csv\"), index=False)\n",
        "print(f\"Per-activity metrics saved to: {os.path.join(OUTPUT_DIR, 'per_activity_metrics.csv')}\")\n",
        "\n",
        "# Save per-lifecycle metrics\n",
        "lifecycle_metrics = []\n",
        "for lifecycle in encoder.target_lifecycle_encoder.classes_:\n",
        "    mask = y_true_lc == lifecycle\n",
        "    if mask.sum() > 0:\n",
        "        precision, recall, f1, support = precision_recall_fscore_support(\n",
        "            y_true_lc[mask], y_pred_lc[mask], average=\"weighted\", zero_division=0\n",
        "        )\n",
        "        acc = accuracy_score(y_true_lc[mask], y_pred_lc[mask])\n",
        "        lifecycle_metrics.append({\n",
        "            \"lifecycle\": lifecycle,\n",
        "            \"accuracy\": acc,\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"f1_score\": f1,\n",
        "            \"support\": mask.sum()\n",
        "        })\n",
        "\n",
        "lifecycle_metrics_df = pd.DataFrame(lifecycle_metrics).sort_values(\"support\", ascending=False)\n",
        "lifecycle_metrics_df.to_csv(os.path.join(OUTPUT_DIR, \"per_lifecycle_metrics.csv\"), index=False)\n",
        "print(f\"Per-lifecycle metrics saved to: {os.path.join(OUTPUT_DIR, 'per_lifecycle_metrics.csv')}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"EVALUATION COMPLETE\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nAll results saved to: {OUTPUT_DIR}\")\n",
        "print(f\"\\nFiles generated:\")\n",
        "print(f\"  - summary_metrics.csv\")\n",
        "print(f\"  - detailed_predictions.csv\")\n",
        "print(f\"  - per_activity_metrics.csv\")\n",
        "print(f\"  - per_lifecycle_metrics.csv\")\n",
        "print(f\"  - confusion_matrix_activities.png\")\n",
        "print(f\"  - confusion_matrix_lifecycles.png\")\n",
        "print(f\"  - top_k_accuracy.png\")\n",
        "print(f\"  - per_activity_accuracy.png\")\n",
        "print(f\"  - accuracy_by_position.png\")\n",
        "print(f\"\\nCompleted: {datetime.now()}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
