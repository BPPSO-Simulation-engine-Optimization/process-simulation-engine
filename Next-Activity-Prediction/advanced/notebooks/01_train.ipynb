{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training Pipeline\n",
        "\n",
        "Complete pipeline for data preparation and LSTM model training for all decision points.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "sys.path.insert(0, os.path.dirname(os.getcwd()))\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from pm4py.objects.bpmn.importer import importer as bpmn_importer\n",
        "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
        "from pm4py.objects.conversion.log import converter as log_converter\n",
        "\n",
        "from preprocessing import DecisionPointExtractor, TrainingDataGenerator\n",
        "from models import SequenceEncoder, ContextEncoder, LSTMPredictor\n",
        "from storage import ModelPersistence\n",
        "\n",
        "pd.options.mode.chained_assignment = None\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "BPMN_PATH = os.path.join(os.getcwd(), \"..\", \"..\", \"..\", \"process_model\", \"loan_application.bpmn\")\n",
        "XES_PATH = os.path.join(os.getcwd(), \"..\", \"..\", \"..\", \"Dataset\", \"BPI Challenge 2017.xes\")\n",
        "MODELS_DIR = os.path.join(os.getcwd(), \"..\", \"models_lstm_new\")\n",
        "DATA_DIR = os.path.join(os.getcwd(), \"..\", \"data\", \"processed\")\n",
        "\n",
        "CONTEXT_KEYS = [\"case:LoanGoal\", \"case:ApplicationType\", \"case:RequestedAmount\"]\n",
        "MAX_HISTORY = 15\n",
        "MIN_SEQ_COUNT = 20\n",
        "MIN_CLASS_COUNT = 10\n",
        "RANDOM_STATE = 42\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading BPMN: d:\\Repos\\process-simulation-engine\\Next-Activity-Prediction\\advanced\\notebooks\\..\\..\\..\\process_model\\loan_application.bpmn\n",
            "Loading event log: d:\\Repos\\process-simulation-engine\\Next-Activity-Prediction\\advanced\\notebooks\\..\\..\\..\\Dataset\\BPI Challenge 2017.xes\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "parsing log, completed traces :: 100%|██████████| 31509/31509 [00:36<00:00, 854.72it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 1202267 events from 31509 cases\n"
          ]
        }
      ],
      "source": [
        "print(f\"Loading BPMN: {BPMN_PATH}\")\n",
        "bpmn_model = bpmn_importer.apply(BPMN_PATH)\n",
        "\n",
        "print(f\"Loading event log: {XES_PATH}\")\n",
        "event_log = xes_importer.apply(XES_PATH)\n",
        "df_log = log_converter.apply(event_log, variant=log_converter.Variants.TO_DATA_FRAME)\n",
        "\n",
        "print(f\"Loaded {len(df_log)} events from {df_log['case:concept:name'].nunique()} cases\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Extract Decision Points\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 49 decision points\n",
            "  DP 1: 1 incoming, 5 outgoing\n",
            "  DP 2: 2 incoming, 1 outgoing\n",
            "  DP 3: 1 incoming, 5 outgoing\n"
          ]
        }
      ],
      "source": [
        "extractor = DecisionPointExtractor(bpmn_model)\n",
        "dp_map = extractor.extract()\n",
        "\n",
        "print(f\"Found {len(dp_map)} decision points\")\n",
        "for dp, config in list(dp_map.items())[:3]:\n",
        "    print(f\"  {dp}: {len(config['incoming'])} incoming, {len(config['outgoing'])} outgoing\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Prepare Event Log\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prepared 1233776 events with End markers\n"
          ]
        }
      ],
      "source": [
        "columns = [\n",
        "    \"case:concept:name\", \"concept:name\", \"org:resource\",\n",
        "    \"time:timestamp\", \"case:LoanGoal\", \"case:ApplicationType\", \"case:RequestedAmount\"\n",
        "]\n",
        "\n",
        "df = df_log[columns].copy()\n",
        "df[\"time:timestamp\"] = pd.to_datetime(df[\"time:timestamp\"])\n",
        "df = df.sort_values([\"case:concept:name\", \"time:timestamp\"]).reset_index(drop=True)\n",
        "\n",
        "end_events = df.sort_values(\"time:timestamp\").groupby(\"case:concept:name\").tail(1).copy()\n",
        "end_events[\"time:timestamp\"] += pd.Timedelta(seconds=1)\n",
        "end_events[\"concept:name\"] = \"End\"\n",
        "\n",
        "df = pd.concat([df, end_events], ignore_index=True)\n",
        "df = df.sort_values([\"case:concept:name\", \"time:timestamp\"]).reset_index(drop=True)\n",
        "\n",
        "print(f\"Prepared {len(df)} events with End markers\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Generate Training Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DP 1: 31509 samples, 3 classes\n",
            "DP 2: no valid samples\n",
            "DP 3: 47079 samples, 2 classes\n",
            "DP 4: 31441 samples, 2 classes\n",
            "DP 5: 31441 samples, 2 classes\n",
            "DP 6: no valid samples\n",
            "DP 7: 113969 samples, 4 classes\n",
            "DP 8: 145410 samples, 4 classes\n",
            "DP 9: 179311 samples, 4 classes\n",
            "DP 10: 179311 samples, 4 classes\n",
            "DP 11: 179256 samples, 3 classes\n",
            "DP 12: 179256 samples, 3 classes\n",
            "DP 13: 178967 samples, 2 classes\n",
            "DP 14: 36213 samples, 2 classes\n",
            "DP 15: 36213 samples, 2 classes\n",
            "DP 16: 33084 samples, 6 classes\n",
            "DP 17: 33084 samples, 6 classes\n",
            "DP 18: no valid samples\n",
            "DP 19: 179857 samples, 7 classes\n",
            "DP 20: 179857 samples, 7 classes\n",
            "DP 21: 179786 samples, 6 classes\n",
            "DP 22: 179786 samples, 6 classes\n",
            "DP 23: 70745 samples, 7 classes\n",
            "DP 24: no valid samples\n",
            "DP 25: 5469 samples, 5 classes\n",
            "DP 26: 73711 samples, 7 classes\n",
            "DP 27: 426576 samples, 8 classes\n",
            "DP 28: 426576 samples, 8 classes\n",
            "DP 29: 426576 samples, 8 classes\n",
            "DP 30: 426576 samples, 8 classes\n",
            "DP 31: 185464 samples, 8 classes\n",
            "DP 32: 426576 samples, 8 classes\n",
            "DP 33: 35961 samples, 3 classes\n",
            "DP 34: 426576 samples, 8 classes\n",
            "DP 35: 426576 samples, 8 classes\n",
            "DP 36: 426576 samples, 8 classes\n",
            "DP 37: 426576 samples, 8 classes\n",
            "DP 38: no valid samples\n",
            "DP 39: 149508 samples, 8 classes\n",
            "DP 40: 149314 samples, 7 classes\n",
            "DP 41: 149314 samples, 7 classes\n",
            "DP 42: 426576 samples, 8 classes\n",
            "DP 43: 426576 samples, 8 classes\n",
            "DP 44: 25366 samples, 3 classes\n",
            "DP 45: no valid samples\n",
            "DP 46: 5469 samples, 5 classes\n",
            "DP 47: 20032 samples, 5 classes\n",
            "DP 48: 426576 samples, 8 classes\n",
            "DP 49: 426576 samples, 8 classes\n",
            "\n",
            "Generated datasets for 43 decision points\n"
          ]
        }
      ],
      "source": [
        "dp_datasets = {}\n",
        "\n",
        "for dp_name, dp_config in dp_map.items():\n",
        "    generator = TrainingDataGenerator(\n",
        "        df, \n",
        "        {dp_name: dp_config},\n",
        "        max_history=MAX_HISTORY,\n",
        "        min_seq_count=MIN_SEQ_COUNT,\n",
        "        min_class_count=MIN_CLASS_COUNT\n",
        "    )\n",
        "    result = generator.generate()\n",
        "    \n",
        "    if dp_name in result:\n",
        "        dp_datasets[dp_name] = result[dp_name]\n",
        "        print(f\"{dp_name}: {len(result[dp_name])} samples, {result[dp_name]['label'].nunique()} classes\")\n",
        "    else:\n",
        "        print(f\"{dp_name}: no valid samples\")\n",
        "\n",
        "print(f\"\\nGenerated datasets for {len(dp_datasets)} decision points\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Split Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DP 1: train=22056, test=6302, holdout=3151\n",
            "DP 3: train=32955, test=9416, holdout=4708\n",
            "DP 4: train=22008, test=6288, holdout=3145\n",
            "DP 5: train=22008, test=6288, holdout=3145\n",
            "DP 7: train=79778, test=22794, holdout=11397\n",
            "DP 8: train=101787, test=29082, holdout=14541\n",
            "DP 9: train=125517, test=35862, holdout=17932\n",
            "DP 10: train=125517, test=35862, holdout=17932\n",
            "DP 11: train=125478, test=35852, holdout=17926\n",
            "DP 12: train=125478, test=35852, holdout=17926\n",
            "DP 13: train=125276, test=35794, holdout=17897\n",
            "DP 14: train=25348, test=7243, holdout=3622\n",
            "DP 15: train=25348, test=7243, holdout=3622\n",
            "DP 16: train=23158, test=6617, holdout=3309\n",
            "DP 17: train=23158, test=6617, holdout=3309\n",
            "DP 19: train=125899, test=35972, holdout=17986\n",
            "DP 20: train=125899, test=35972, holdout=17986\n",
            "DP 21: train=125849, test=35958, holdout=17979\n",
            "DP 22: train=125849, test=35958, holdout=17979\n",
            "DP 23: train=49521, test=14149, holdout=7075\n",
            "DP 25: train=3828, test=1094, holdout=547\n",
            "DP 26: train=51597, test=14742, holdout=7372\n",
            "DP 27: train=298602, test=85316, holdout=42658\n",
            "DP 28: train=298602, test=85316, holdout=42658\n",
            "DP 29: train=298602, test=85316, holdout=42658\n",
            "DP 30: train=298602, test=85316, holdout=42658\n",
            "DP 31: train=129824, test=37093, holdout=18547\n",
            "DP 32: train=298602, test=85316, holdout=42658\n",
            "DP 33: train=25172, test=7192, holdout=3597\n",
            "DP 34: train=298602, test=85316, holdout=42658\n",
            "DP 35: train=298602, test=85316, holdout=42658\n",
            "DP 36: train=298602, test=85316, holdout=42658\n",
            "DP 37: train=298602, test=85316, holdout=42658\n",
            "DP 39: train=104655, test=29902, holdout=14951\n",
            "DP 40: train=104519, test=29863, holdout=14932\n",
            "DP 41: train=104519, test=29863, holdout=14932\n",
            "DP 42: train=298602, test=85316, holdout=42658\n",
            "DP 43: train=298602, test=85316, holdout=42658\n",
            "DP 44: train=17755, test=5074, holdout=2537\n",
            "DP 46: train=3828, test=1094, holdout=547\n",
            "DP 47: train=14021, test=4007, holdout=2004\n",
            "DP 48: train=298602, test=85316, holdout=42658\n",
            "DP 49: train=298602, test=85316, holdout=42658\n"
          ]
        }
      ],
      "source": [
        "splits = {}\n",
        "\n",
        "for dp, data in dp_datasets.items():\n",
        "    data = data.drop(columns=[\"sequence_timestamps\"], errors=\"ignore\")\n",
        "    \n",
        "    temp, holdout = train_test_split(\n",
        "        data, test_size=0.1, random_state=RANDOM_STATE,\n",
        "        stratify=data[\"label\"] if data[\"label\"].nunique() > 1 else None\n",
        "    )\n",
        "    train, test = train_test_split(\n",
        "        temp, test_size=2/9, random_state=RANDOM_STATE,\n",
        "        stratify=temp[\"label\"] if temp[\"label\"].nunique() > 1 else None\n",
        "    )\n",
        "    \n",
        "    splits[dp] = {\n",
        "        \"train\": train.reset_index(drop=True),\n",
        "        \"test\": test.reset_index(drop=True),\n",
        "        \"holdout\": holdout.reset_index(drop=True)\n",
        "    }\n",
        "    print(f\"{dp}: train={len(train)}, test={len(test)}, holdout={len(holdout)}\")\n",
        "\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "joblib.dump(splits, os.path.join(DATA_DIR, \"dp_split_datasets_full_simple.joblib\"))\n",
        "print(f\"\\nSaved splits to {DATA_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Train Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Training DP 13 ===\n",
            "Epoch 1/10\n",
            "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.9382 - loss: 0.1846 - val_accuracy: 0.9540 - val_loss: 0.1711\n",
            "Epoch 2/10\n",
            "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9620 - loss: 0.1457 - val_accuracy: 0.9579 - val_loss: 0.1617\n",
            "Epoch 3/10\n",
            "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.9635 - loss: 0.1413 - val_accuracy: 0.9568 - val_loss: 0.1677\n",
            "Epoch 4/10\n",
            "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9641 - loss: 0.1405 - val_accuracy: 0.9590 - val_loss: 0.1623\n",
            "Epoch 5/10\n",
            "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.9642 - loss: 0.1384 - val_accuracy: 0.9601 - val_loss: 0.1620\n",
            "Epoch 6/10\n",
            "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.9643 - loss: 0.1378 - val_accuracy: 0.9576 - val_loss: 0.1624\n",
            "Epoch 7/10\n",
            "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.9644 - loss: 0.1369 - val_accuracy: 0.9590 - val_loss: 0.1592\n",
            "Epoch 8/10\n",
            "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.9647 - loss: 0.1355 - val_accuracy: 0.9593 - val_loss: 0.1601\n",
            "Epoch 8: early stopping\n",
            "Restoring model weights from the end of the best epoch: 5.\n",
            "\n",
            "Trained 1 models\n"
          ]
        }
      ],
      "source": [
        "trained_models = {}\n",
        "\n",
        "for dp, split in splits.items():\n",
        "    print(f\"\\n=== Training {dp} ===\")\n",
        "    \n",
        "    df_pool = pd.concat([split[\"train\"], split[\"test\"]], ignore_index=True)\n",
        "    df_all = pd.concat([df_pool, split[\"holdout\"]], ignore_index=True)\n",
        "    n_pool = len(df_pool)\n",
        "    \n",
        "    seq_encoder = SequenceEncoder()\n",
        "    X_acts, X_durs, X_res, y = seq_encoder.fit_transform(df_all)\n",
        "    seq_encoder.add_unknown_token()\n",
        "    \n",
        "    ctx_encoder = ContextEncoder(CONTEXT_KEYS)\n",
        "    X_ctx = ctx_encoder.fit_transform(df_all)\n",
        "    \n",
        "    X_acts_train, X_acts_test = X_acts[:n_pool], X_acts[n_pool:]\n",
        "    X_durs_train, X_durs_test = X_durs[:n_pool], X_durs[n_pool:]\n",
        "    X_res_train, X_res_test = X_res[:n_pool], X_res[n_pool:]\n",
        "    X_ctx_train, X_ctx_test = X_ctx[:n_pool], X_ctx[n_pool:]\n",
        "    y_train, y_test = y[:n_pool], y[n_pool:]\n",
        "    \n",
        "    predictor = LSTMPredictor(\n",
        "        num_activities=seq_encoder.num_activities,\n",
        "        num_resources=seq_encoder.num_resources,\n",
        "        context_dim=ctx_encoder.dim,\n",
        "        max_seq_len=seq_encoder.max_seq_len,\n",
        "        num_classes=seq_encoder.num_classes\n",
        "    )\n",
        "    predictor.build()\n",
        "    predictor.train(\n",
        "        [X_acts_train, X_durs_train, X_res_train, X_ctx_train],\n",
        "        y_train\n",
        "    )\n",
        "    \n",
        "    trained_models[dp] = {\n",
        "        \"model\": predictor.model,\n",
        "        \"activity_encoder\": seq_encoder.activity_encoder,\n",
        "        \"resource_encoder\": seq_encoder.resource_encoder,\n",
        "        \"label_encoder\": seq_encoder.label_encoder,\n",
        "        \"context_encoders\": ctx_encoder.encoders,\n",
        "        \"context_keys\": CONTEXT_KEYS,\n",
        "        \"max_seq_len\": seq_encoder.max_seq_len\n",
        "    }\n",
        "\n",
        "print(f\"\\nTrained {len(trained_models)} models\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Save Models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved all models to d:\\Repos\\process-simulation-engine\\Next-Activity-Prediction\\advanced\\notebooks\\..\\models_lstm_new\n"
          ]
        }
      ],
      "source": [
        "ModelPersistence.save_all(trained_models, MODELS_DIR)\n",
        "print(f\"Saved all models to {MODELS_DIR}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
